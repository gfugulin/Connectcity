#!/usr/bin/env python3
"""
Script para testar o sistema completo de validaÃ§Ã£o
"""
import asyncio
import sys
import os
import json
import logging
from pathlib import Path
from datetime import datetime

# Adicionar diretÃ³rio raiz ao path
sys.path.append(str(Path(__file__).parent.parent))

from integration.sp_data_validator import SPDataValidator, ValidationLevel
from integration.integration_tests import SPIntegrationTests

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def test_validator_initialization():
    """Testa inicializaÃ§Ã£o do validador"""
    try:
        logger.info("ğŸ”§ Testando inicializaÃ§Ã£o do validador...")
        
        validator = SPDataValidator()
        
        # Verificar configuraÃ§Ãµes
        stats = validator.get_validation_stats()
        rules = validator.get_validation_rules()
        
        logger.info(f"âœ… Validador inicializado - {len(rules)} regras ativas")
        return validator
        
    except Exception as e:
        logger.error(f"âŒ Erro na inicializaÃ§Ã£o do validador: {e}")
        return None

async def test_gtfs_validation(validator):
    """Testa validaÃ§Ã£o de dados GTFS"""
    try:
        logger.info("ğŸ” Testando validaÃ§Ã£o GTFS...")
        
        # Dados de teste GTFS
        test_gtfs_data = {
            'stops': [
                {
                    'stop_id': '1',
                    'stop_name': 'EstaÃ§Ã£o SÃ©',
                    'stop_lat': -23.5505,
                    'stop_lon': -46.6333
                },
                {
                    'stop_id': '2',
                    'stop_name': 'EstaÃ§Ã£o Paulista',
                    'stop_lat': -23.5614,
                    'stop_lon': -46.6565
                },
                {
                    'stop_id': '3',
                    'stop_name': 'Terminal Bandeira',
                    'stop_lat': -23.5500,
                    'stop_lon': -46.6300
                }
            ],
            'routes': [
                {
                    'route_id': '1',
                    'route_short_name': '107P',
                    'route_long_name': 'Terminal Bandeira - EstaÃ§Ã£o SÃ©'
                },
                {
                    'route_id': '2',
                    'route_short_name': '175P',
                    'route_long_name': 'Terminal Bandeira - EstaÃ§Ã£o Paulista'
                }
            ],
            'trips': [
                {
                    'trip_id': '1',
                    'route_id': '1',
                    'service_id': '1'
                }
            ],
            'stop_times': [
                {
                    'trip_id': '1',
                    'stop_id': '1',
                    'arrival_time': '06:00:00',
                    'departure_time': '06:00:00'
                }
            ]
        }
        
        # Validar dados
        result = await validator.validate_gtfs_data(test_gtfs_data)
        
        logger.info(f"ğŸ“Š Resultado da validaÃ§Ã£o GTFS:")
        logger.info(f"  - VÃ¡lido: {result.valid}")
        logger.info(f"  - Score: {result.score:.2f}")
        logger.info(f"  - Erros: {len(result.errors)}")
        logger.info(f"  - Warnings: {len(result.warnings)}")
        logger.info(f"  - Info: {len(result.info)}")
        
        if result.errors:
            for error in result.errors:
                logger.warning(f"  âŒ {error}")
        
        if result.warnings:
            for warning in result.warnings:
                logger.warning(f"  âš ï¸ {warning}")
        
        if result.info:
            for info in result.info:
                logger.info(f"  â„¹ï¸ {info}")
        
        return result.valid and result.score >= 0.7
        
    except Exception as e:
        logger.error(f"âŒ Erro na validaÃ§Ã£o GTFS: {e}")
        return False

async def test_osm_validation(validator):
    """Testa validaÃ§Ã£o de dados OSM"""
    try:
        logger.info("ğŸ” Testando validaÃ§Ã£o OSM...")
        
        # Dados de teste OSM
        test_osm_data = {
            'nodes': [
                {
                    'id': '1',
                    'lat': -23.5505,
                    'lon': -46.6333,
                    'tags': {'highway': 'bus_stop', 'name': 'EstaÃ§Ã£o SÃ©'}
                },
                {
                    'id': '2',
                    'lat': -23.5614,
                    'lon': -46.6565,
                    'tags': {'highway': 'bus_stop', 'name': 'EstaÃ§Ã£o Paulista'}
                }
            ],
            'ways': [
                {
                    'id': '1',
                    'nodes': ['1', '2'],
                    'tags': {
                        'highway': 'primary',
                        'name': 'Avenida Paulista',
                        'wheelchair': 'yes',
                        'tactile_paving': 'yes'
                    }
                },
                {
                    'id': '2',
                    'nodes': ['2'],
                    'tags': {
                        'highway': 'secondary',
                        'name': 'Rua da ConsolaÃ§Ã£o',
                        'wheelchair': 'no'
                    }
                }
            ],
            'relations': []
        }
        
        # Validar dados
        result = await validator.validate_osm_data(test_osm_data)
        
        logger.info(f"ğŸ“Š Resultado da validaÃ§Ã£o OSM:")
        logger.info(f"  - VÃ¡lido: {result.valid}")
        logger.info(f"  - Score: {result.score:.2f}")
        logger.info(f"  - Erros: {len(result.errors)}")
        logger.info(f"  - Warnings: {len(result.warnings)}")
        logger.info(f"  - Info: {len(result.info)}")
        
        if result.errors:
            for error in result.errors:
                logger.warning(f"  âŒ {error}")
        
        if result.warnings:
            for warning in result.warnings:
                logger.warning(f"  âš ï¸ {warning}")
        
        if result.info:
            for info in result.info:
                logger.info(f"  â„¹ï¸ {info}")
        
        return result.valid and result.score >= 0.7
        
    except Exception as e:
        logger.error(f"âŒ Erro na validaÃ§Ã£o OSM: {e}")
        return False

async def test_integrated_validation(validator):
    """Testa validaÃ§Ã£o de dados integrados"""
    try:
        logger.info("ğŸ” Testando validaÃ§Ã£o de dados integrados...")
        
        # Dados de teste integrados
        test_integrated_data = {
            'nodes': [
                {
                    'id': '1',
                    'type': 'gtfs_stop',
                    'lat': -23.5505,
                    'lon': -46.6333,
                    'name': 'EstaÃ§Ã£o SÃ©',
                    'accessibility': 'accessible'
                },
                {
                    'id': '2',
                    'type': 'osm_node',
                    'lat': -23.5614,
                    'lon': -46.6565,
                    'name': 'EstaÃ§Ã£o Paulista',
                    'accessibility': 'accessible'
                }
            ],
            'edges': [
                {
                    'id': '1',
                    'type': 'gtfs_route',
                    'from_node': '1',
                    'to_node': '2',
                    'duration': 30,
                    'accessibility': 'accessible'
                },
                {
                    'id': '2',
                    'type': 'osm_way',
                    'from_node': '1',
                    'to_node': '2',
                    'distance': 5.0,
                    'accessibility': 'accessible'
                }
            ]
        }
        
        # Validar dados
        result = await validator.validate_integrated_data(test_integrated_data)
        
        logger.info(f"ğŸ“Š Resultado da validaÃ§Ã£o integrada:")
        logger.info(f"  - VÃ¡lido: {result.valid}")
        logger.info(f"  - Score: {result.score:.2f}")
        logger.info(f"  - Erros: {len(result.errors)}")
        logger.info(f"  - Warnings: {len(result.warnings)}")
        logger.info(f"  - Info: {len(result.info)}")
        
        if result.errors:
            for error in result.errors:
                logger.warning(f"  âŒ {error}")
        
        if result.warnings:
            for warning in result.warnings:
                logger.warning(f"  âš ï¸ {warning}")
        
        if result.info:
            for info in result.info:
                logger.info(f"  â„¹ï¸ {info}")
        
        return result.valid and result.score >= 0.7
        
    except Exception as e:
        logger.error(f"âŒ Erro na validaÃ§Ã£o integrada: {e}")
        return False

async def test_integration_tests():
    """Testa testes de integraÃ§Ã£o"""
    try:
        logger.info("ğŸ§ª Testando testes de integraÃ§Ã£o...")
        
        integration_tests = SPIntegrationTests()
        
        # Executar teste completo
        suite = await integration_tests.run_full_integration_test()
        
        logger.info(f"ğŸ“Š Resultado dos testes de integraÃ§Ã£o:")
        logger.info(f"  - Score total: {suite.total_score:.2f}")
        logger.info(f"  - Total de testes: {len(suite.tests)}")
        logger.info(f"  - DuraÃ§Ã£o: {(suite.end_time - suite.start_time).total_seconds():.2f}s")
        
        # Mostrar resultados individuais
        for test in suite.tests:
            status_emoji = "âœ…" if test.status.value == "passed" else "âŒ"
            logger.info(f"  {status_emoji} {test.name}: {test.status.value} (Score: {test.score:.2f})")
            
            if test.errors:
                for error in test.errors:
                    logger.warning(f"    âŒ {error}")
            
            if test.warnings:
                for warning in test.warnings:
                    logger.warning(f"    âš ï¸ {warning}")
            
            if test.info:
                for info in test.info:
                    logger.info(f"    â„¹ï¸ {info}")
        
        # Exportar resultados
        export_file = "test_validation_results.json"
        await integration_tests.export_test_results(suite, export_file)
        logger.info(f"ğŸ“ Resultados exportados para: {export_file}")
        
        return suite.total_score >= 0.7
        
    except Exception as e:
        logger.error(f"âŒ Erro nos testes de integraÃ§Ã£o: {e}")
        return False

async def test_validation_rules(validator):
    """Testa regras de validaÃ§Ã£o"""
    try:
        logger.info("ğŸ“‹ Testando regras de validaÃ§Ã£o...")
        
        # Obter regras
        rules = validator.get_validation_rules()
        
        logger.info(f"ğŸ“Š Regras de validaÃ§Ã£o disponÃ­veis: {len(rules)}")
        
        for rule in rules:
            logger.info(f"  - {rule['name']}: {rule['description']}")
            logger.info(f"    NÃ­vel: {rule['level']}, Peso: {rule['weight']}, Ativa: {rule['enabled']}")
        
        # Testar habilitaÃ§Ã£o/desabilitaÃ§Ã£o de regras
        validator.disable_validation_rule('naming_conventions')
        validator.enable_validation_rule('naming_conventions')
        
        logger.info("âœ… Regras de validaÃ§Ã£o testadas")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Erro no teste de regras: {e}")
        return False

async def test_validation_levels(validator):
    """Testa diferentes nÃ­veis de validaÃ§Ã£o"""
    try:
        logger.info("ğŸ“Š Testando diferentes nÃ­veis de validaÃ§Ã£o...")
        
        levels = [ValidationLevel.STRICT, ValidationLevel.MODERATE, ValidationLevel.LENIENT]
        
        for level in levels:
            logger.info(f"ğŸ” Testando nÃ­vel: {level.value}")
            validator.update_validation_level(level)
            
            # Verificar se foi aplicado
            current_level = validator.validation_level
            logger.info(f"  NÃ­vel atual: {current_level.value}")
        
        # Voltar para moderado
        validator.update_validation_level(ValidationLevel.MODERATE)
        logger.info("âœ… NÃ­veis de validaÃ§Ã£o testados")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Erro no teste de nÃ­veis: {e}")
        return False

async def test_validation_stats(validator):
    """Testa estatÃ­sticas de validaÃ§Ã£o"""
    try:
        logger.info("ğŸ“ˆ Testando estatÃ­sticas de validaÃ§Ã£o...")
        
        # Obter estatÃ­sticas
        stats = validator.get_validation_stats()
        
        logger.info(f"ğŸ“Š EstatÃ­sticas de validaÃ§Ã£o:")
        logger.info(f"  - Total de validaÃ§Ãµes: {stats['total_validations']}")
        logger.info(f"  - ValidaÃ§Ãµes bem-sucedidas: {stats['successful_validations']}")
        logger.info(f"  - ValidaÃ§Ãµes falharam: {stats['failed_validations']}")
        logger.info(f"  - Taxa de sucesso: {stats['success_rate']:.2f}")
        logger.info(f"  - Score mÃ©dio: {stats['avg_score']:.2f}")
        logger.info(f"  - NÃ­vel de validaÃ§Ã£o: {stats['validation_level']}")
        logger.info(f"  - Regras ativas: {stats['active_rules']}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Erro no teste de estatÃ­sticas: {e}")
        return False

async def main():
    """FunÃ§Ã£o principal de teste"""
    try:
        logger.info("ğŸ§ª === TESTE COMPLETO DE VALIDAÃ‡ÃƒO ===")
        
        # Teste 1: InicializaÃ§Ã£o do validador
        validator = await test_validator_initialization()
        if not validator:
            logger.error("âŒ Falha na inicializaÃ§Ã£o - abortando testes")
            return False
        
        # Teste 2: ValidaÃ§Ã£o GTFS
        success1 = await test_gtfs_validation(validator)
        
        # Teste 3: ValidaÃ§Ã£o OSM
        success2 = await test_osm_validation(validator)
        
        # Teste 4: ValidaÃ§Ã£o integrada
        success3 = await test_integrated_validation(validator)
        
        # Teste 5: Testes de integraÃ§Ã£o
        success4 = await test_integration_tests()
        
        # Teste 6: Regras de validaÃ§Ã£o
        success5 = await test_validation_rules(validator)
        
        # Teste 7: NÃ­veis de validaÃ§Ã£o
        success6 = await test_validation_levels(validator)
        
        # Teste 8: EstatÃ­sticas
        success7 = await test_validation_stats(validator)
        
        # Resumo dos testes
        tests = [
            ("InicializaÃ§Ã£o", True),
            ("ValidaÃ§Ã£o GTFS", success1),
            ("ValidaÃ§Ã£o OSM", success2),
            ("ValidaÃ§Ã£o Integrada", success3),
            ("Testes de IntegraÃ§Ã£o", success4),
            ("Regras de ValidaÃ§Ã£o", success5),
            ("NÃ­veis de ValidaÃ§Ã£o", success6),
            ("EstatÃ­sticas", success7)
        ]
        
        logger.info("\nğŸ“Š === RESUMO DOS TESTES ===")
        passed = 0
        for test_name, success in tests:
            status = "âœ… PASSOU" if success else "âŒ FALHOU"
            logger.info(f"{test_name}: {status}")
            if success:
                passed += 1
        
        logger.info(f"\nğŸ¯ Resultado: {passed}/{len(tests)} testes passaram")
        
        if passed == len(tests):
            logger.info("ğŸ‰ TODOS OS TESTES PASSARAM!")
            logger.info("âœ… Sistema de validaÃ§Ã£o funcionando corretamente")
            return True
        else:
            logger.warning(f"âš ï¸ {len(tests) - passed} testes falharam")
            return False
        
    except Exception as e:
        logger.error(f"âŒ Erro nos testes: {e}")
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
